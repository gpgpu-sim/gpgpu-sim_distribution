# see README before running this

# to run the debug build of GPGPU-Sim run:
# source setup_environment debug
if [ $# == '1' ] ;
then
    export GPGPUSIM_CONFIG=$1
else
    export GPGPUSIM_CONFIG=release
fi

export GPGPUSIM_ROOT="$( cd "$( dirname "$BASH_SOURCE" )" && pwd )"

export QTINC=/usr/include
export BOOST_ROOT=/usr/include/boost
export BOOST_LIB=/usr/lib64
export BOOST_VER=

# change this to point to your opencl library directory, usually /usr/lib
# or /usr/lib64. Not setting this variable will cause gpgpu-sim to build
# without opencl support.
export NVOPENCL_LIBDIR=

# change this to point to your opencl include directory, usually
# $CUDA_INSTALL_PATH/include. Not setting this variable will cause
# gpgpu-sim to build without opencl support.
export NVOPENCL_INCDIR=

# change this to point to your installation of the NVIDIA compute SDK
export NVIDIA_COMPUTE_SDK_LOCATION=$HOME/NVIDIA_GPU_Computing_SDK/

# change the following to point to your installation of CUDA Toolkit
export CUDA_INSTALL_PATH=/usr/local/cuda/

# setting LD_LIBRARY_PATH as follows enables GPGPU-Sim to be invoked by 
# native CUDA and OpenCL applications. GPGPU-Sim is dynamically linked
# against instead of the CUDA toolkit.  This replaces this cumbersome
# static link setup in prior GPGPU-Sim releases.
export LD_LIBRARY_PATH=`echo $LD_LIBRARY_PATH | sed -re 's/.*gpgpu_sim.*(release|debug)://'`
export LD_LIBRARY_PATH=$GPGPUSIM_ROOT/lib/$GPGPUSIM_CONFIG:$LD_LIBRARY_PATH
export PATH=$GPGPUSIM_ROOT/bin:$CUDA_INSTALL_PATH/bin:$PATH

# The following sets OPENCL_REMOTE_GPU_HOST which is used by GPGPU-Sim to
# SSH to remote node to generate PTX for OpenCL kernels when running on 
# a node that does not have an NVIDIA driver installed.
# The remote node should have GPGPU-Sim installed at the same path 
if [ `hostname -d` == "ece.ubc.ca" ]; then
	export OPENCL_REMOTE_GPU_HOST=aamodt-pc05.ece.ubc.ca
	if [ `hostname -f` == $OPENCL_REMOTE_GPU_HOST ]; then
		unset OPENCL_REMOTE_GPU_HOST
	fi
fi
