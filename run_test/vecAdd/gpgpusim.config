# This config models the Pascal GP102 (GeForceGTX 1080Ti)

# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 20 

# SASS execution (only supported with CUDA >= 4.0)
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0

# high level architecture configuration
-gpgpu_n_clusters 28
-gpgpu_n_cores_per_cluster 1
-gpgpu_n_mem 11
-gpgpu_n_sub_partition_per_mchannel 2 

# Pascal clock domains
#-gpgpu_clock_domains <Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>
# Pascal NVIDIA TITAN X clock domains are adopted from 
# https://en.wikipedia.org/wiki/GeForce_10_series
-gpgpu_clock_domains 1481.0:2962.0:1481.0:2750.0

# shader core pipeline config
-gpgpu_shader_registers 65536

# This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 32
-gpgpu_simd_model 1 

# Pipeline widths and number of FUs
# ID_OC_SP,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_SFU,OC_EX_MEM,EX_WB
## Pascal GP102 has 4 SP SIMD units and 1 SFU unit
## we need to scale the number of pipeline registers to be equal to the number of SP units
-gpgpu_pipeline_widths 4,0,0,1,1,4,0,0,1,1,6
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 1

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
# SFU is 32-width in pascal, then dp units initiation is 1 cycle
-ptx_opcode_latency_int 4,13,4,5,145,32
-ptx_opcode_initiation_int 1,1,1,1,4,4
-ptx_opcode_latency_fp 4,13,4,5,39
-ptx_opcode_initiation_fp 1,2,1,1,4
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 1,2,1,1,130
-ptx_opcode_initiation_sfu 4
-ptx_opcode_latency_sfu 20

# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
# Note: Hashing set index function (H) only applies to a set size of 32 or 64. 
# Pascal GP102 has 96KB Shared memory
# Pascal GP102 has 64KB L1 cache
# The default is to disable the L1 cache, unless cache modifieres is used
-gpgpu_l1_banks 2
-gpgpu_cache:dl1  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_cache:dl1PrefL1  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_cache:dl1PrefShared  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_shmem_size 98304
-gpgpu_shmem_sizeDefault 98304
-gpgpu_shmem_size_PrefL1 98304
-gpgpu_shmem_size_PrefShared 98304
# By default, L1 cache is disabled in Pascal P102.
# requests with .nc modifier or __ldg mehtod will be cached in L1 cache even with gmem_skip_L1D=1
-gpgpu_gmem_skip_L1D 1
-icnt_flit_size 40
-gpgpu_n_cluster_ejection_buffer_size 32
-gpgpu_l1_latency 82
-gpgpu_smem_latency 24
-gpgpu_flush_l1_cache 1

# 64 sets, each 128 bytes 16-way for each memory sub partition (128 KB per memory sub partition). This gives 3MB L2 cache
-gpgpu_cache:dl2 S:64:128:16,L:B:m:L:P,A:256:64,16:0,32
-gpgpu_cache:dl2_texture_only 0 
-gpgpu_dram_partition_queues 32:32:32:32
-gpgpu_perf_sim_memcpy 1
-gpgpu_memory_partition_indexing 4

# 4 KB Inst.
-gpgpu_cache:il1 N:8:128:4,L:R:f:N:L,S:2:48,4
-gpgpu_inst_fetch_throughput 8

# 48 KB Tex 
-gpgpu_tex_cache:l1 N:16:128:24,L:R:m:N:L,T:128:4,128:2
# 12 KB Const
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,S:2:64,4
-gpgpu_perfect_inst_const_cache 1

# enable operand collector 
## larger operand collectors and reg_banks are needed for the 4 warp schedulers and 4 SIMD units
-gpgpu_operand_collector_num_units_sp 20
-gpgpu_operand_collector_num_units_sfu 4
-gpgpu_operand_collector_num_units_mem 8
-gpgpu_operand_collector_num_in_ports_sp 4
-gpgpu_operand_collector_num_out_ports_sp 4
-gpgpu_operand_collector_num_in_ports_sfu 1
-gpgpu_operand_collector_num_out_ports_sfu 1
-gpgpu_operand_collector_num_in_ports_mem 1
-gpgpu_operand_collector_num_out_ports_mem 1
# gpgpu_num_reg_banks should be increased to 32, but it gives an error!
-gpgpu_num_reg_banks 32

# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1

## In Pascal, a warp scheduler can issue 2 insts per cycle
-gpgpu_max_insn_issue_per_warp 2

# interconnection
-network_mode 1 
-inter_config_file config_fermi_islip.icnt

# memory partition latency config 
-gpgpu_l2_rop_latency 120
-dram_latency 100

# dram model config
-gpgpu_dram_scheduler 1
# The DRAM return queue and the scheduler queue together should provide buffer
# to sustain the memory level parallelism to tolerate DRAM latency 
# To allow 100% DRAM utility, there should at least be enough buffer to sustain
# the minimum DRAM latency (100 core cycles).  I.e. 
#   Total buffer space required = 100 x 924MHz / 700MHz = 132
-gpgpu_frfcfs_dram_sched_queue_size 64
-gpgpu_dram_return_queue_size 116

# for NVIDIA GeForceGTX 1080Ti, bus width is 352bits (11 DRAM chips x 32 bits)
# 11 memory paritions, 4 bytes (1 DRAM chip) per memory partition
# the atom size of GDDR5X (the smallest read request) is 32 bytes 
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 4
-gpgpu_dram_burst_length 8
-dram_data_command_freq_ratio 4  # GDDR5X is QDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS

# Use the same GDDR5 timing from hynix H5GQ1H24AFR
# disable bank groups for now, set nbkgrp to 1 and tCCDL and tRTPL to 0
-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40:
                        CL=12:WL=4:CDLR=5:WR=12:nbkgrp=1:CCDL=0:RTPL=0"

# Pascal has four schedulers per core
-gpgpu_num_sched_per_core 2
# Two Level Scheduler with active and pending pools
#-gpgpu_scheduler two_level_active:6:0:1
# Loose round robbin scheduler
#-gpgpu_scheduler lrr
# Greedy then oldest scheduler
-gpgpu_scheduler gto

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# power model configs
-power_simulation_enabled 0
#-gpuwattch_xml_file gpuwattch_geforcegtx1080ti.xml

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0

### items for functional and timing simulation of UVM ###

# gddr size should be less than or equal to 1GB, in the unit of MB/GB
-gddr_size 1GB

# size of gddr page, only 4KB and 2MB available
-page_size 4KB

# number of tlb entries per SM
-tlb_size 4096

# average page table walk latency (in core cycle)
# for 4K page, set to 100 and for 2M page, set to 66
-page_table_walk_latency 100

# page eviction policy
# 0 - lru 2MB (default)
# 1 - lru tree-based neighborhood
# 2 - lru sequential locality 64K
# 3 - random 4KB
# 4 - LFU 2MB
# 5 - lru 4KB
-eviction_policy 0

# invalidate clean pages directly instead of writing back
-invalidate_clean 0

# reserve percentage (e.g. 10 or 20) of accesses pages from eviction in hope that they will be accessed in next iteration
-reserve_accessed_page_percent 0

# percentage of free page buffer to trigger the page eviction (e.g. 5 or 10)
-percentage_of_free_page_buffer 0

# pcie bandwidth per direction
-pcie_bandwidth 16.0GB/s

# enable/disable GMMU statistics profiling for UVM
-sim_prof_enable 1

# disable deadlock check for UVM
-gpgpu_deadlock_detect 0

# latency to process kernel launch (150us or 222150 core cycles)
#-gpgpu_kernel_launch_latency 222150

# hardware prefetcher
# 0 - disabled
# 1 - tree-based neighborhood (default)
# 2 - sequential locality 64K
# 3 - random 4 K
-hardware_prefetch 1

# hardware prefetcher under over-subscription
# 0 - disable upon eviction (default)
# 1 - tree-based neighborhood
# 2 - sequential locality 64K
# 3 - random 4 K
-hwprefetch_oversub 1

# latency in core cycle to handle page fault (45us)
# encompass the overhead of stalling threads, deciding memory address, page table walk, maintaining page flags, transfer chunks and orders
-page_fault_latency 66645

# enabling accurate simulation for stalling warps and serializing accesses for page fault handling (default 0)
-enable_accurate_simulation 0

# Enable direct CPU-memory access from GPU
# 0 - disable
# 1 - adaptive
# 2 - always
# 3 - after oversubscription
-enable_dma 0

# Access counter threshold for migrating the page from cpu to gpu
-migrate_threshold 8

# Oversubscription Multiplicative Penalty Factor for Adaptive DMA
-multiply_dma_penalty 2

# enabling access pattern detection, policy engine, and adaptive memory management
-enable_smart_runtime 0

