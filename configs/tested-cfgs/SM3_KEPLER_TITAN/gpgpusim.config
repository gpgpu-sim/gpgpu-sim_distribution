# This config models the KEPLER (TITAN)
# For more info about this card, see Nvidia White paper
# https://wr0.wr.inf.h-brs.de/wr/hardware/nodes3/nvidia/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf

# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 35 
-gpgpu_ignore_resources_limitation 1

# Device Limits
-gpgpu_stack_size_limit 1024
-gpgpu_heap_size_limit 8388608
-gpgpu_runtime_sync_depth_limit 2
-gpgpu_runtime_pending_launch_count_limit 2048

# Compute Capability
-gpgpu_compute_capability_major 3
-gpgpu_compute_capability_minor 5

# PTX execution-driven
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0
# SASS trace-driven mode execution
#-trace_driven_mode 1

# high level architecture configuration
-gpgpu_n_clusters 14
-gpgpu_n_cores_per_cluster 1
-gpgpu_n_mem 12
-gpgpu_n_sub_partition_per_mchannel 2 

# Kepler clock domains
#-gpgpu_clock_domains <Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>
# Kepler NVIDIA TITAN clock domains are adopted from 
# https://en.wikipedia.org/wiki/GeForce_700_series
-gpgpu_clock_domains 837.0:837.0:837.0:1502.0

# shader core pipeline config
-gpgpu_shader_registers 65536
-gpgpu_occupancy_sm_number 62

# This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 16
-gpgpu_simd_model 1 

# Pipeline widths and number of FUs
# ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB
## Kepler has 6 SP SIMD units, 4 DPs and 2 SFU units per SM. 
# There is no INT unit in kepler
-gpgpu_pipeline_widths 6,4,0,2,1,6,4,0,2,1,12
-gpgpu_num_sp_units 6
-gpgpu_num_sfu_units 2
-gpgpu_num_dp_units 4

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
# All Div operations are executed on SFU unit
# Throughput (initiation latency) are adopted from CUDA SDK document V8, section 5.4.1, Table 2
-ptx_opcode_latency_int 4,13,4,5,145,32
-ptx_opcode_initiation_int 1,1,1,1,4,4
-ptx_opcode_latency_fp 4,13,4,5,39
-ptx_opcode_initiation_fp 1,2,1,1,4
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 2,8,8,8,130
-ptx_opcode_initiation_sfu 2
-ptx_opcode_latency_sfu 200

# enable operand collector 
-gpgpu_operand_collector_num_units_sp 12
-gpgpu_operand_collector_num_units_sfu 6
-gpgpu_operand_collector_num_units_mem 8
-gpgpu_operand_collector_num_units_dp 6
-gpgpu_operand_collector_num_in_ports_sp 2
-gpgpu_operand_collector_num_out_ports_sp 2
-gpgpu_operand_collector_num_in_ports_sfu 2
-gpgpu_operand_collector_num_out_ports_sfu 2
-gpgpu_operand_collector_num_in_ports_mem 1
-gpgpu_operand_collector_num_out_ports_mem 1
-gpgpu_operand_collector_num_in_ports_dp 1
-gpgpu_operand_collector_num_out_ports_dp 1
-gpgpu_num_reg_banks 16
-gpgpu_reg_file_port_throughput 2

# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1
# Use kepler Coalsce arhitetecture
-gpgpu_coalesce_arch 35

## In Kepler, a warp scheduler can issue 2 insts per cycle
-gpgpu_max_insn_issue_per_warp 2
-gpgpu_dual_issue_diff_exec_units 0

# Kepler TITAN has four schedulers per core
-gpgpu_num_sched_per_core 4
# Greedy then oldest scheduler
-gpgpu_scheduler gto

# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
# Note: Hashing set index function (H) only applies to a set size of 32 or 64. 
# The defulat is to disable the L1 cache, unless cache modifieres are used
-gpgpu_cache:dl1  S:4:128:32,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_cache:dl1PrefL1  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_cache:dl1PrefShared  S:4:128:32,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_shmem_size 49152
-gpgpu_shmem_sizeDefault 49152
-gpgpu_shmem_size_PrefL1 16384
-gpgpu_shmem_size_PrefShared 49152
# By default, L1 cache is disabled in Kepler P102 and only enabled for local memory
# requests with .nc modifier or __ldg mehtod will be cached in L1 cache even with gmem_skip_L1D=1
-gpgpu_gmem_skip_L1D 1
-icnt_flit_size 40
-gpgpu_n_cluster_ejection_buffer_size 32
-gpgpu_l1_latency 82
-smem_latency 24
-gpgpu_flush_l1_cache 1

# 32 sets, each 128 bytes 16-way for each memory sub partition (128 KB per memory sub partition). This gives 1.5MB L2 cache
-gpgpu_cache:dl2 S:32:128:16,L:B:m:L:P,A:256:64,16:0,32
-gpgpu_cache:dl2_texture_only 0 
-gpgpu_dram_partition_queues 32:32:32:32
-gpgpu_perf_sim_memcpy 1
-gpgpu_memory_partition_indexing 0

# 4 KB Inst.
-gpgpu_cache:il1 N:8:128:4,L:R:f:N:L,S:2:48,4
-gpgpu_inst_fetch_throughput 8
# 48 KB Tex 
-gpgpu_tex_cache:l1 N:16:128:24,L:R:m:N:L,T:128:4,128:2
# 12 KB Const
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,S:2:64,4

# interconnection
-network_mode 1 
-inter_config_file config_kepler_islip.icnt

# memory partition latency config 
-gpgpu_l2_rop_latency 120
-dram_latency 100

# dram model config
-gpgpu_dram_scheduler 1
-gpgpu_frfcfs_dram_sched_queue_size 64
-gpgpu_dram_return_queue_size 64

# for NVIDIA TITAN, bus width is 384bits (12 DRAM chips x 32 bits)
# 12 memory paritions, 4 bytes (1 DRAM chip) per memory partition
# the atom size of GDDR5X (the smallest read request) is 32 bytes 
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 4
-gpgpu_dram_burst_length 8
-dram_data_command_freq_ratio 4  # GDDR5X is QDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS

# Use the same GDDR5 timing, scaled to 2500MHZ
-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=8:RCD=16:RAS=37:RP=16:RC=52:
                        CL=16:WL=6:CDLR=7:WR=16:nbkgrp=4:CCDL=4:RTPL=3"

-dram_bnk_indexing_policy 0
-dram_bnkgrp_indexing_policy 1

#-dram_seperate_write_queue_enable 1
#-dram_write_queue_size 64:56:32

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# power model configs, disable it untill we create a real energy model for Kepler
-power_simulation_enabled 0

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0

